{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a36c564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Documents-OCR-STS-2 to yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5743/5743 [00:01<00:00, 3973.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Documents-OCR-STS-2 in yolov8:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:00<00:00, 2836.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"4t8iWiXww6199WuWDQXD\")\n",
    "project = rf.workspace(\"kealfeyne\").project(\"documents-ocr-sts\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b701471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import easyocr\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "836e8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.119 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=../Master-Documents-OCR/datasets/data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=train26, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/runs/detect/train26\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "YOLOv8n summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 149.3¬±49.7 MB/s, size: 58.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/datasets/train/labels.cache... 192 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 192/192 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.1¬±0.1 ms, read: 196.2¬±50.7 MB/s, size: 67.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/datasets/valid/labels.cache... 13 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/runs/detect/train26/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/runs/detect/train26\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30       7.3G      5.927      6.017      4.285         67        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [02:13<00:00, 11.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:40<00:00, 40.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13         42          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      7.29G      5.632      5.907      4.213         38        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:59<00:00,  9.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13         42          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      7.28G      5.087      5.395      4.062         50        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:36<00:00,  8.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13         42          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      7.28G      4.551      4.656      3.727         50        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:34<00:00,  7.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13         42          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      7.29G      3.888      3.999      3.247         63        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:29<00:00,  7.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13         42          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      7.28G      3.394        3.7       3.04         83        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:23<00:00,  6.95s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 2.650s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:08<00:00,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         13         42          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      7.28G      3.247      3.431      2.755         66        640:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 [01:04<00:46,  9.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[274]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m PATH_TO_DATASET = \u001b[33m\"\u001b[39m\u001b[33m../Master-Documents-OCR/datasets\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.yaml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPATH_TO_DATASET\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/data.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:790\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    787\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    789\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:210\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:384\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m.amp):\n\u001b[32m    383\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.preprocess_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:119\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    107\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:301\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m    300\u001b[39m preds = \u001b[38;5;28mself\u001b[39m.forward(batch[\u001b[33m\"\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/utils/loss.py:259\u001b[39m, in \u001b[36mv8DetectionLoss.__call__\u001b[39m\u001b[34m(self, preds, batch)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fg_mask.sum():\n\u001b[32m    258\u001b[39m     target_bboxes /= stride_tensor\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     loss[\u001b[32m0\u001b[39m], loss[\u001b[32m2\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbbox_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpred_distri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_scores_sum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfg_mask\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m loss[\u001b[32m0\u001b[39m] *= \u001b[38;5;28mself\u001b[39m.hyp.box  \u001b[38;5;66;03m# box gain\u001b[39;00m\n\u001b[32m    264\u001b[39m loss[\u001b[32m1\u001b[39m] *= \u001b[38;5;28mself\u001b[39m.hyp.cls  \u001b[38;5;66;03m# cls gain\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/utils/loss.py:115\u001b[39m, in \u001b[36mBboxLoss.forward\u001b[39m\u001b[34m(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dfl_loss:\n\u001b[32m    114\u001b[39m     target_ltrb = bbox2dist(anchor_points, target_bboxes, \u001b[38;5;28mself\u001b[39m.dfl_loss.reg_max - \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     loss_dfl = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdfl_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_dist\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfg_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdfl_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreg_max\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ltrb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfg_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m * weight\n\u001b[32m    116\u001b[39m     loss_dfl = loss_dfl.sum() / target_scores_sum\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/ultralytics/utils/loss.py:93\u001b[39m, in \u001b[36mDFLoss.__call__\u001b[39m\u001b[34m(self, pred_dist, target)\u001b[39m\n\u001b[32m     90\u001b[39m wl = tr - target  \u001b[38;5;66;03m# weight left\u001b[39;00m\n\u001b[32m     91\u001b[39m wr = \u001b[32m1\u001b[39m - wl  \u001b[38;5;66;03m# weight right\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.view(tl.shape) * wl\n\u001b[32m     94\u001b[39m     + F.cross_entropy(pred_dist, tr.view(-\u001b[32m1\u001b[39m), reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m).view(tl.shape) * wr\n\u001b[32m     95\u001b[39m ).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "PATH_TO_DATASET = \"../Master-Documents-OCR/datasets\"\n",
    "\n",
    "model = YOLO('yolov8n.yaml')\n",
    "\n",
    "results = model.train(data=f'{PATH_TO_DATASET}/data.yaml',\n",
    "                      epochs=30,\n",
    "                      imgsz=640,\n",
    "                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "dfce5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(\"runs/detect/train23/weights/best.pt\")\n",
    "\n",
    "# metrics = model.val(split='val',\n",
    "#                     imgsz=640,\n",
    "#                     device=device,\n",
    "#                     iou=0.75)\n",
    "# metrics.box.map  # map50-95\n",
    "# metrics.box.map50  # map50\n",
    "# metrics.box.map75  # map75\n",
    "# metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "53f35b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    def __init__(self, path_to_weights: str = 'weights/detector.pt', confidence_level: float = 0.25):\n",
    "        self.model = YOLO(path_to_weights)\n",
    "        self.model.conf = confidence_level\n",
    "\n",
    "    def predict(self, image: np.array):\n",
    "        outputs = self.model.predict(image, task=\"detection\", max_det=4)[0]\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "                {\"class\": outputs.boxes.cls.numpy().tolist(),\n",
    "                 'name': [outputs.names[x] for x in outputs.boxes.cls.numpy().tolist()],\n",
    "                 \"confidence_level\": outputs.boxes.conf.numpy().tolist(),\n",
    "                 \"xyxy\": outputs.boxes.xyxy.numpy().tolist(),\n",
    "                 \"image\": [image[int(y1):int(y2), int(x1):int(x2)] for (x1, y1, x2, y2) in outputs.boxes.xyxy.numpy().tolist()]\n",
    "                }\n",
    "            )\n",
    "\n",
    "        indices = df[[\"class\", \"confidence_level\"]].groupby(by=\"class\").idxmax()[\"confidence_level\"].values\n",
    "        predictions = df.loc[indices].to_dict(\"records\")\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "348fad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recognitor:\n",
    "    def __init__(self, ):\n",
    "        self.model = easyocr.Reader(['ru'], gpu=False)\n",
    "\n",
    "    def predict(self, image: np.array):\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        cv2.imshow(\"image\", gray_image)\n",
    "        # cv2.waitKey()\n",
    "        predictions = self.model.recognize(gray_image)[0]\n",
    "\n",
    "        return predictions[1] # –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e625bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    detector = Detector(\"runs/detect/train22/weights/best.pt\")\n",
    "    recognitor = Recognitor()\n",
    "\n",
    "    detections = detector.predict(image)\n",
    "\n",
    "    for detection in detections:\n",
    "        detection[\"text\"] = recognitor.predict(detection[\"image\"])\n",
    "\n",
    "    return detections\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ee79173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame({\n",
    "    \"path\": [\n",
    "        \"REAL_STS_15_png_part_0_jpg.rf.df20c6faeddb30cd3976893937f96eb0.jpg\",\n",
    "        \"REAL_STS_18_png_part_0_jpg.rf.f76e628af811430039c16cb847bdb12c.jpg\",\n",
    "        \"REAL_STS_45_png_part_0_jpg.rf.95e321353311ec3aaa98c6079bf1c948.jpg\",\n",
    "        \"REAL_STS_56_png_part_0_jpg.rf.191238a1fc3914287a5ad70534600bfe.jpg\",\n",
    "        \"REAL_STS_96_png_part_0_jpg.rf.426c2bf5c5c1b7903756d2e76b5bf549.jpg\",\n",
    "        \"STS_4_part_1_jpg.rf.9437e8eaaab990d8be1c973bfaff4458.jpg\",\n",
    "        \"STS_12_part_0_jpg.rf.e143c80521c3297baa6553ac92e96e82.jpg\",\n",
    "        \"STS_21_part_1_jpg.rf.526b388bc6949f98ec6b5b3266390735.jpg\",\n",
    "        \"STS_28_part_0_jpg.rf.d692eac8ce0d2713f344690693cdf706.jpg\",\n",
    "        \"STS_45_part_0_jpg.rf.3b323eb2aeec6781ed4778655b26f011.jpg\",\n",
    "        \"STS_57_part_0_jpg.rf.8c5964bfce4e3634de84b5cffbb6164b.jpg\",\n",
    "        \"STS_60_part_0_jpg.rf.d205baf635138c8edc64f12e3011156b.jpg\",\n",
    "        \"STS_65_part_0_jpg.rf.bcc912c74c6a93f79a2c86e7bfb79d16.jpg\"\n",
    "    ],\n",
    "    \"series\": [\n",
    "        \"3449\",\n",
    "        \"3452\",\n",
    "        \"5250\",\n",
    "        \"5717\",\n",
    "        \"9906\",\n",
    "        \"77–£–¢\",\n",
    "        \"4227\",\n",
    "        \"1845\",\n",
    "        \"78–•–°\",\n",
    "        \"68–°–ù\",\n",
    "        \"50–†–•\",\n",
    "        \"9917\",\n",
    "        \"2344\"\n",
    "    ],\n",
    "    \"number\": [\n",
    "        \"518592\",\n",
    "        \"191266\",\n",
    "        \"652301\",\n",
    "        \"943303\",\n",
    "        \"098900\",\n",
    "        \"243401\",\n",
    "        \"286592\",\n",
    "        \"136217\",\n",
    "        \"494489\",\n",
    "        \"436211\",\n",
    "        \"328818\",\n",
    "        \"947265\",\n",
    "        \"306076\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4597aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 number, 1 series, 65.1ms\n",
      "Speed: 2.5ms preprocess, 65.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '518592', 'series': '34 49'}\n",
      "\n",
      "0: 640x640 2 numbers, 2 seriess, 57.4ms\n",
      "Speed: 1.8ms preprocess, 57.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '191266', 'series': '34 52'}\n",
      "\n",
      "0: 640x640 2 numbers, 2 seriess, 56.6ms\n",
      "Speed: 1.2ms preprocess, 56.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '6 5 2 3 0 1', 'series': '52 50'}\n",
      "\n",
      "0: 640x640 2 numbers, 2 seriess, 66.7ms\n",
      "Speed: 1.7ms preprocess, 66.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': \"'9 4 3 3 0 \", 'series': '57 17'}\n",
      "\n",
      "0: 640x640 2 numbers, 2 seriess, 65.9ms\n",
      "Speed: 1.6ms preprocess, 65.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '09890 (', 'series': '9 9 0 6'}\n",
      "\n",
      "0: 640x640 2 numbers, 1 series, 54.9ms\n",
      "Speed: 1.6ms preprocess, 54.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '243401', 'series': '77, '}\n",
      "\n",
      "0: 640x640 1 number, 1 series, 57.9ms\n",
      "Speed: 1.1ms preprocess, 57.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '286592', 'series': '42 27'}\n",
      "\n",
      "0: 640x640 2 numbers, 1 series, 49.4ms\n",
      "Speed: 1.5ms preprocess, 49.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '136217', 'series': '18 45'}\n",
      "\n",
      "0: 640x640 2 numbers, 2 seriess, 47.1ms\n",
      "Speed: 1.4ms preprocess, 47.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '64 94489', 'series': '7 8 –• –™'}\n",
      "\n",
      "0: 640x640 1 number, 48.3ms\n",
      "Speed: 1.2ms preprocess, 48.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '436211'}\n",
      "\n",
      "0: 640x640 1 number, 49.0ms\n",
      "Speed: 1.2ms preprocess, 49.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': '319 9 81 ,'}\n",
      "\n",
      "0: 640x640 2 numbers, 1 series, 59.8ms\n",
      "Speed: 1.1ms preprocess, 59.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "{'number': '9 4 72 6 !', 'series': '99 1 7'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 number, 1 series, 71.5ms\n",
      "Speed: 1.7ms preprocess, 71.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "{'number': '306076', 'series': '23 44'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "series_predictions = []\n",
    "number_predictions = []\n",
    "\n",
    "for path in labels[\"path\"]:\n",
    "    image = cv2.imread(f'datasets/valid/images/{path}')\n",
    "    preds = pd.DataFrame(predict(image)).groupby(\"name\").max()[\"text\"].to_dict()\n",
    "    print(preds)\n",
    "\n",
    "    series_predictions.append(re.sub(r\"[^–ê-–Ø0-9]\", \"\", preds.get(\"series\", \"\")).upper())\n",
    "    number_predictions.append(re.sub(r\"[^–ê-–Ø0-9]\", \"\", preds.get(\"number\", \"\")).upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9c8323b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>series</th>\n",
       "      <th>number</th>\n",
       "      <th>series_prediction</th>\n",
       "      <th>number_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REAL_STS_15_png_part_0_jpg.rf.df20c6faeddb30cd...</td>\n",
       "      <td>3449</td>\n",
       "      <td>518592</td>\n",
       "      <td>3449</td>\n",
       "      <td>518592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REAL_STS_18_png_part_0_jpg.rf.f76e628af8114300...</td>\n",
       "      <td>3452</td>\n",
       "      <td>191266</td>\n",
       "      <td>3452</td>\n",
       "      <td>191266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REAL_STS_45_png_part_0_jpg.rf.95e321353311ec3a...</td>\n",
       "      <td>5250</td>\n",
       "      <td>652301</td>\n",
       "      <td>5250</td>\n",
       "      <td>652301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REAL_STS_56_png_part_0_jpg.rf.191238a1fc391428...</td>\n",
       "      <td>5717</td>\n",
       "      <td>943303</td>\n",
       "      <td>5717</td>\n",
       "      <td>94330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REAL_STS_96_png_part_0_jpg.rf.426c2bf5c5c1b790...</td>\n",
       "      <td>9906</td>\n",
       "      <td>098900</td>\n",
       "      <td>9906</td>\n",
       "      <td>09890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>STS_4_part_1_jpg.rf.9437e8eaaab990d8be1c973bfa...</td>\n",
       "      <td>77–£–¢</td>\n",
       "      <td>243401</td>\n",
       "      <td>77</td>\n",
       "      <td>243401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>STS_12_part_0_jpg.rf.e143c80521c3297baa6553ac9...</td>\n",
       "      <td>4227</td>\n",
       "      <td>286592</td>\n",
       "      <td>4227</td>\n",
       "      <td>286592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>STS_21_part_1_jpg.rf.526b388bc6949f98ec6b5b326...</td>\n",
       "      <td>1845</td>\n",
       "      <td>136217</td>\n",
       "      <td>1845</td>\n",
       "      <td>136217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>STS_28_part_0_jpg.rf.d692eac8ce0d2713f34469069...</td>\n",
       "      <td>78–•–°</td>\n",
       "      <td>494489</td>\n",
       "      <td>78–•–™</td>\n",
       "      <td>6494489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>STS_45_part_0_jpg.rf.3b323eb2aeec6781ed4778655...</td>\n",
       "      <td>68–°–ù</td>\n",
       "      <td>436211</td>\n",
       "      <td></td>\n",
       "      <td>436211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>STS_57_part_0_jpg.rf.8c5964bfce4e3634de84b5cff...</td>\n",
       "      <td>50–†–•</td>\n",
       "      <td>328818</td>\n",
       "      <td></td>\n",
       "      <td>319981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>STS_60_part_0_jpg.rf.d205baf635138c8edc64f12e3...</td>\n",
       "      <td>9917</td>\n",
       "      <td>947265</td>\n",
       "      <td>9917</td>\n",
       "      <td>94726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>STS_65_part_0_jpg.rf.bcc912c74c6a93f79a2c86e7b...</td>\n",
       "      <td>2344</td>\n",
       "      <td>306076</td>\n",
       "      <td>2344</td>\n",
       "      <td>306076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path series  number  \\\n",
       "0   REAL_STS_15_png_part_0_jpg.rf.df20c6faeddb30cd...   3449  518592   \n",
       "1   REAL_STS_18_png_part_0_jpg.rf.f76e628af8114300...   3452  191266   \n",
       "2   REAL_STS_45_png_part_0_jpg.rf.95e321353311ec3a...   5250  652301   \n",
       "3   REAL_STS_56_png_part_0_jpg.rf.191238a1fc391428...   5717  943303   \n",
       "4   REAL_STS_96_png_part_0_jpg.rf.426c2bf5c5c1b790...   9906  098900   \n",
       "5   STS_4_part_1_jpg.rf.9437e8eaaab990d8be1c973bfa...   77–£–¢  243401   \n",
       "6   STS_12_part_0_jpg.rf.e143c80521c3297baa6553ac9...   4227  286592   \n",
       "7   STS_21_part_1_jpg.rf.526b388bc6949f98ec6b5b326...   1845  136217   \n",
       "8   STS_28_part_0_jpg.rf.d692eac8ce0d2713f34469069...   78–•–°  494489   \n",
       "9   STS_45_part_0_jpg.rf.3b323eb2aeec6781ed4778655...   68–°–ù  436211   \n",
       "10  STS_57_part_0_jpg.rf.8c5964bfce4e3634de84b5cff...   50–†–•  328818   \n",
       "11  STS_60_part_0_jpg.rf.d205baf635138c8edc64f12e3...   9917  947265   \n",
       "12  STS_65_part_0_jpg.rf.bcc912c74c6a93f79a2c86e7b...   2344  306076   \n",
       "\n",
       "   series_prediction number_prediction  \n",
       "0               3449            518592  \n",
       "1               3452            191266  \n",
       "2               5250            652301  \n",
       "3               5717             94330  \n",
       "4               9906             09890  \n",
       "5                 77            243401  \n",
       "6               4227            286592  \n",
       "7               1845            136217  \n",
       "8               78–•–™           6494489  \n",
       "9                               436211  \n",
       "10                              319981  \n",
       "11              9917             94726  \n",
       "12              2344            306076  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[\"series_prediction\"] = series_predictions\n",
    "labels[\"number_prediction\"] = number_predictions\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "05989240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.615384615384617"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 0\n",
    "error = 0\n",
    "\n",
    "for enumerate, row in labels.iterrows():\n",
    "    length += len(row[\"number\"])\n",
    "    error += nltk.edit_distance(row[\"number\"], row[\"number_prediction\"], substitution_cost=1, transpositions=False)\n",
    "\n",
    "    length += len(row[\"series\"])\n",
    "    error += nltk.edit_distance(row[\"series\"], row[\"series_prediction\"], substitution_cost=1, transpositions=False)\n",
    "\n",
    "error / length * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9c002485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 number, 1 series, 43.2ms\n",
      "Speed: 1.8ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "path = \"–ö–æ–ø–∏—è STS Polina.png\"\n",
    "\n",
    "model = YOLO('runs/detect/train22/weights/best.pt')\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = model([path])\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    result.show()  # display to screen\n",
    "    # result.save(filename='result.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 numbers, 2 seriess, 52.4ms\n",
      "Speed: 1.1ms preprocess, 52.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[1 0]\n",
      "[{'name': 'number', 'value': '191266'}, {'name': 'series', 'value': '34 52'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.shkolin/VSCodeProjects/Master-Documents-OCR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# image = cv2.imread(f'datasets/valid/images/{path}')\n",
    "# print(predict(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19bcedd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([[np.int32(1828), np.int32(114)],\n",
       "   [np.int32(2144), np.int32(114)],\n",
       "   [np.int32(2144), np.int32(166)],\n",
       "   [np.int32(1828), np.int32(166)]],\n",
       "  'BOAR DING',\n",
       "  np.float64(0.5610812465513767)),\n",
       " ([[np.int32(2162), np.int32(114)],\n",
       "   [np.int32(2318), np.int32(114)],\n",
       "   [np.int32(2318), np.int32(166)],\n",
       "   [np.int32(2162), np.int32(166)]],\n",
       "  'PASS',\n",
       "  np.float64(0.999984860420227)),\n",
       " ([[np.int32(1832), np.int32(232)],\n",
       "   [np.int32(1936), np.int32(232)],\n",
       "   [np.int32(1936), np.int32(264)],\n",
       "   [np.int32(1832), np.int32(264)]],\n",
       "  'FROM:',\n",
       "  np.float64(0.9998976936254952)),\n",
       " ([[np.int32(2113), np.int32(229)],\n",
       "   [np.int32(2169), np.int32(229)],\n",
       "   [np.int32(2169), np.int32(265)],\n",
       "   [np.int32(2113), np.int32(265)]],\n",
       "  '–¢–û:',\n",
       "  np.float64(0.5245268503855097)),\n",
       " ([[np.int32(222), np.int32(249)],\n",
       "   [np.int32(423), np.int32(249)],\n",
       "   [np.int32(423), np.int32(313)],\n",
       "   [np.int32(222), np.int32(313)]],\n",
       "  'FROM:',\n",
       "  np.float64(0.9999857123679332)),\n",
       " ([[np.int32(1103), np.int32(251)],\n",
       "   [np.int32(1207), np.int32(251)],\n",
       "   [np.int32(1207), np.int32(313)],\n",
       "   [np.int32(1103), np.int32(313)]],\n",
       "  '–¢O:',\n",
       "  np.float64(0.5007377209174544)),\n",
       " ([[np.int32(1826), np.int32(277)],\n",
       "   [np.int32(1982), np.int32(277)],\n",
       "   [np.int32(1982), np.int32(353)],\n",
       "   [np.int32(1826), np.int32(353)]],\n",
       "  'JFK',\n",
       "  np.float64(0.9999960769466525)),\n",
       " ([[np.int32(2115), np.int32(277)],\n",
       "   [np.int32(2279), np.int32(277)],\n",
       "   [np.int32(2279), np.int32(351)],\n",
       "   [np.int32(2115), np.int32(351)]],\n",
       "  'LHR',\n",
       "  np.float64(0.9999927733243802)),\n",
       " ([[np.int32(1833), np.int32(353)],\n",
       "   [np.int32(2039), np.int32(353)],\n",
       "   [np.int32(2039), np.int32(393)],\n",
       "   [np.int32(1833), np.int32(393)]],\n",
       "  'NEW YORK',\n",
       "  np.float64(0.44385536595063774)),\n",
       " ([[np.int32(2113), np.int32(353)],\n",
       "   [np.int32(2277), np.int32(353)],\n",
       "   [np.int32(2277), np.int32(393)],\n",
       "   [np.int32(2113), np.int32(393)]],\n",
       "  'LONDON',\n",
       "  np.float64(0.999999805332041)),\n",
       " ([[np.int32(213), np.int32(338)],\n",
       "   [np.int32(520), np.int32(338)],\n",
       "   [np.int32(520), np.int32(487)],\n",
       "   [np.int32(213), np.int32(487)]],\n",
       "  'JFK',\n",
       "  np.float64(0.9999986234898187)),\n",
       " ([[np.int32(1102), np.int32(340)],\n",
       "   [np.int32(1421), np.int32(340)],\n",
       "   [np.int32(1421), np.int32(481)],\n",
       "   [np.int32(1102), np.int32(481)]],\n",
       "  'LHR',\n",
       "  np.float64(0.9999576724280396)),\n",
       " ([[np.int32(1829), np.int32(413)],\n",
       "   [np.int32(2031), np.int32(413)],\n",
       "   [np.int32(2031), np.int32(449)],\n",
       "   [np.int32(1829), np.int32(449)]],\n",
       "  'Jan 07,2025',\n",
       "  np.float64(0.9173508179743645)),\n",
       " ([[np.int32(2113), np.int32(413)],\n",
       "   [np.int32(2315), np.int32(413)],\n",
       "   [np.int32(2315), np.int32(449)],\n",
       "   [np.int32(2113), np.int32(449)]],\n",
       "  'Jan 07,2025',\n",
       "  np.float64(0.6337788475576679)),\n",
       " ([[np.int32(1829), np.int32(447)],\n",
       "   [np.int32(1965), np.int32(447)],\n",
       "   [np.int32(1965), np.int32(483)],\n",
       "   [np.int32(1829), np.int32(483)]],\n",
       "  '11:30 AM',\n",
       "  np.float64(0.6090811665140963)),\n",
       " ([[np.int32(2113), np.int32(447)],\n",
       "   [np.int32(2265), np.int32(447)],\n",
       "   [np.int32(2265), np.int32(483)],\n",
       "   [np.int32(2113), np.int32(483)]],\n",
       "  '06:35 –†M',\n",
       "  np.float64(0.7472462604190702)),\n",
       " ([[np.int32(224), np.int32(484)],\n",
       "   [np.int32(618), np.int32(484)],\n",
       "   [np.int32(618), np.int32(562)],\n",
       "   [np.int32(224), np.int32(562)]],\n",
       "  'NEW YORK',\n",
       "  np.float64(0.8363905353194031)),\n",
       " ([[np.int32(1102), np.int32(488)],\n",
       "   [np.int32(1414), np.int32(488)],\n",
       "   [np.int32(1414), np.int32(562)],\n",
       "   [np.int32(1102), np.int32(562)]],\n",
       "  'LONDON',\n",
       "  np.float64(0.9999102003436708)),\n",
       " ([[np.int32(1826), np.int32(540)],\n",
       "   [np.int32(1997), np.int32(540)],\n",
       "   [np.int32(1997), np.int32(583)],\n",
       "   [np.int32(1826), np.int32(583)]],\n",
       "  'Passenger',\n",
       "  np.float64(0.7111591998844207)),\n",
       " ([[np.int32(2173), np.int32(539)],\n",
       "   [np.int32(2273), np.int32(539)],\n",
       "   [np.int32(2273), np.int32(583)],\n",
       "   [np.int32(2173), np.int32(583)]],\n",
       "  'Flight',\n",
       "  np.float64(0.8622616416507112)),\n",
       " ([[np.int32(1825), np.int32(577)],\n",
       "   [np.int32(1995), np.int32(577)],\n",
       "   [np.int32(1995), np.int32(613)],\n",
       "   [np.int32(1825), np.int32(613)]],\n",
       "  'JOHN DOE',\n",
       "  np.float64(0.99851932963069)),\n",
       " ([[np.int32(2175), np.int32(577)],\n",
       "   [np.int32(2287), np.int32(577)],\n",
       "   [np.int32(2287), np.int32(613)],\n",
       "   [np.int32(2175), np.int32(613)]],\n",
       "  '–ê 0123',\n",
       "  np.float64(0.9162501790543227)),\n",
       " ([[np.int32(217), np.int32(601)],\n",
       "   [np.int32(601), np.int32(601)],\n",
       "   [np.int32(601), np.int32(670)],\n",
       "   [np.int32(217), np.int32(670)]],\n",
       "  'Jan 07,2025',\n",
       "  np.float64(0.9998378168585449)),\n",
       " ([[np.int32(1099), np.int32(603)],\n",
       "   [np.int32(1483), np.int32(603)],\n",
       "   [np.int32(1483), np.int32(666)],\n",
       "   [np.int32(1099), np.int32(666)]],\n",
       "  'Jan 07,2025',\n",
       "  np.float64(0.5814993559874287)),\n",
       " ([[np.int32(1826), np.int32(630)],\n",
       "   [np.int32(1904), np.int32(630)],\n",
       "   [np.int32(1904), np.int32(662)],\n",
       "   [np.int32(1826), np.int32(662)]],\n",
       "  'Seat',\n",
       "  np.float64(0.9999996423721313)),\n",
       " ([[np.int32(1986), np.int32(630)],\n",
       "   [np.int32(2064), np.int32(630)],\n",
       "   [np.int32(2064), np.int32(662)],\n",
       "   [np.int32(1986), np.int32(662)]],\n",
       "  'Gate',\n",
       "  np.float64(1.0)),\n",
       " ([[np.int32(2173), np.int32(629)],\n",
       "   [np.int32(2319), np.int32(629)],\n",
       "   [np.int32(2319), np.int32(665)],\n",
       "   [np.int32(2173), np.int32(665)]],\n",
       "  'Terminal',\n",
       "  np.float64(0.9999976397753296)),\n",
       " ([[np.int32(1826), np.int32(666)],\n",
       "   [np.int32(1880), np.int32(666)],\n",
       "   [np.int32(1880), np.int32(698)],\n",
       "   [np.int32(1826), np.int32(698)]],\n",
       "  '17F',\n",
       "  np.float64(0.7441247891893793)),\n",
       " ([[np.int32(1986), np.int32(668)],\n",
       "   [np.int32(2026), np.int32(668)],\n",
       "   [np.int32(2026), np.int32(700)],\n",
       "   [np.int32(1986), np.int32(700)]],\n",
       "  '15',\n",
       "  np.float64(1.0)),\n",
       " ([[np.int32(2175), np.int32(665)],\n",
       "   [np.int32(2225), np.int32(665)],\n",
       "   [np.int32(2225), np.int32(701)],\n",
       "   [np.int32(2175), np.int32(701)]],\n",
       "  '2C',\n",
       "  np.float64(0.5272835662768918)),\n",
       " ([[np.int32(216), np.int32(670)],\n",
       "   [np.int32(473), np.int32(670)],\n",
       "   [np.int32(473), np.int32(735)],\n",
       "   [np.int32(216), np.int32(735)]],\n",
       "  '11:30 AM',\n",
       "  np.float64(0.6593561156836716)),\n",
       " ([[np.int32(1103), np.int32(671)],\n",
       "   [np.int32(1385), np.int32(671)],\n",
       "   [np.int32(1385), np.int32(733)],\n",
       "   [np.int32(1103), np.int32(733)]],\n",
       "  '06.35 –†–ú',\n",
       "  np.float64(0.5152364348342376)),\n",
       " ([[np.int32(219), np.int32(913)],\n",
       "   [np.int32(454), np.int32(913)],\n",
       "   [np.int32(454), np.int32(968)],\n",
       "   [np.int32(219), np.int32(968)]],\n",
       "  'Passenger',\n",
       "  np.float64(0.999999682108536)),\n",
       " ([[np.int32(568), np.int32(907)],\n",
       "   [np.int32(708), np.int32(907)],\n",
       "   [np.int32(708), np.int32(972)],\n",
       "   [np.int32(568), np.int32(972)]],\n",
       "  'Flight',\n",
       "  np.float64(0.869567317460739)),\n",
       " ([[np.int32(841), np.int32(913)],\n",
       "   [np.int32(947), np.int32(913)],\n",
       "   [np.int32(947), np.int32(957)],\n",
       "   [np.int32(841), np.int32(957)]],\n",
       "  'Seat',\n",
       "  np.float64(0.9999746767550305)),\n",
       " ([[np.int32(1063), np.int32(915)],\n",
       "   [np.int32(1169), np.int32(915)],\n",
       "   [np.int32(1169), np.int32(957)],\n",
       "   [np.int32(1063), np.int32(957)]],\n",
       "  'Gate',\n",
       "  np.float64(1.0)),\n",
       " ([[np.int32(1284), np.int32(910)],\n",
       "   [np.int32(1484), np.int32(910)],\n",
       "   [np.int32(1484), np.int32(958)],\n",
       "   [np.int32(1284), np.int32(958)]],\n",
       "  'Terminal',\n",
       "  np.float64(0.6371157778086294)),\n",
       " ([[np.int32(216), np.int32(962)],\n",
       "   [np.int32(452), np.int32(962)],\n",
       "   [np.int32(452), np.int32(1010)],\n",
       "   [np.int32(216), np.int32(1010)]],\n",
       "  'JOHN DOE',\n",
       "  np.float64(0.9999721414791573)),\n",
       " ([[np.int32(574), np.int32(962)],\n",
       "   [np.int32(724), np.int32(962)],\n",
       "   [np.int32(724), np.int32(1010)],\n",
       "   [np.int32(574), np.int32(1010)]],\n",
       "  '–ê 0123',\n",
       "  np.float64(0.6125388841805096)),\n",
       " ([[np.int32(839), np.int32(965)],\n",
       "   [np.int32(915), np.int32(965)],\n",
       "   [np.int32(915), np.int32(1007)],\n",
       "   [np.int32(839), np.int32(1007)]],\n",
       "  '17F',\n",
       "  np.float64(0.7315111501721697)),\n",
       " ([[np.int32(1059), np.int32(965)],\n",
       "   [np.int32(1115), np.int32(965)],\n",
       "   [np.int32(1115), np.int32(1007)],\n",
       "   [np.int32(1059), np.int32(1007)]],\n",
       "  '15',\n",
       "  np.float64(0.9999998314126101)),\n",
       " ([[np.int32(1286), np.int32(962)],\n",
       "   [np.int32(1352), np.int32(962)],\n",
       "   [np.int32(1352), np.int32(1010)],\n",
       "   [np.int32(1286), np.int32(1010)]],\n",
       "  '2C',\n",
       "  np.float64(0.7652709065992527)),\n",
       " ([[np.int32(2012), np.int32(1008)],\n",
       "   [np.int32(2198), np.int32(1008)],\n",
       "   [np.int32(2198), np.int32(1038)],\n",
       "   [np.int32(2012), np.int32(1038)]],\n",
       "  'TELE-PAPER',\n",
       "  np.float64(0.9999868435964292)),\n",
       " ([[np.int32(2010), np.int32(1036)],\n",
       "   [np.int32(2198), np.int32(1036)],\n",
       "   [np.int32(2198), np.int32(1066)],\n",
       "   [np.int32(2010), np.int32(1066)]],\n",
       "  '–£–∑–∏–≥ Bl–∏e   –û—Å–µ–∫^ Partncr',\n",
       "  np.float64(0.5384738161047851)),\n",
       " ([[np.int32(2039), np.int32(1063)],\n",
       "   [np.int32(2197), np.int32(1063)],\n",
       "   [np.int32(2197), np.int32(1077)],\n",
       "   [np.int32(2039), np.int32(1077)]],\n",
       "  'TA Subudfary @–ì–û–ü Graup, J–∞—Ä–∞–ø)',\n",
       "  np.float64(0.27952044877563614))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7057c10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m.no_grad():\n\u001b[32m      2\u001b[39m     outputs = model(**encoding)\n\u001b[32m      3\u001b[39m     predictions = outputs.logits.argmax(-\u001b[32m1\u001b[39m).squeeze().tolist()\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9543e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
